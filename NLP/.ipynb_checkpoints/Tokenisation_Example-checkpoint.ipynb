{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d20e0318-1775-45b7-ad4c-75c73dae455a",
   "metadata": {},
   "source": [
    "## NLTK -> Natural Language Toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81aa2e27-7a88-48d6-8a81-c2aa22a77ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\anonymous\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\anonymous\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\anonymous\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\anonymous\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\anonymous\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\anonymous\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Anonymous\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "628855ab-9179-4894-bf3d-bc1f8ff820a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\" Hello Welcome to Subrat Bahuguna's NLP Tutorial.\n",
    "Please do stick with this notebook.\n",
    "I have made a lot of effort to create it\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a97b42c-18d7-46b2-a7a5-294ee4c97c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello Welcome to Subrat Bahuguna's NLP Tutorial.\n",
      "Please do stick with this notebook.\n",
      "I have made a lot of effort to create it\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026faf8e-7556-463c-b751-f11d07b75ebc",
   "metadata": {},
   "source": [
    "### Sentence Tokenizer (Paragraph/Corpus -> documents/sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "516e95b6-613d-4cff-a55d-45c6a3406b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import punkt\n",
    "from nltk.tokenize import sent_tokenize\n",
    "documents = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25968c9f-1c83-4db1-b4e0-0a0f556b794d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" Hello Welcome to Subrat Bahuguna's NLP Tutorial.\",\n",
       " 'Please do stick with this notebook.',\n",
       " 'I have made a lot of effort to create it']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2e18e7-65ee-411a-9113-e94b90ea1cb5",
   "metadata": {},
   "source": [
    "### Word Tokenizer (Sentence -> Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1608fb4-463e-4845-bd45-b48acc956b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'Subrat',\n",
       " 'Bahuguna',\n",
       " \"'s\",\n",
       " 'NLP',\n",
       " 'Tutorial',\n",
       " '.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'stick',\n",
       " 'with',\n",
       " 'this',\n",
       " 'notebook',\n",
       " '.',\n",
       " 'I',\n",
       " 'have',\n",
       " 'made',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'effort',\n",
       " 'to',\n",
       " 'create',\n",
       " 'it']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word = word_tokenize(corpus)\n",
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d864719-e4d5-41a4-8cf0-06d4685e801e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'Subrat',\n",
       " 'Bahuguna',\n",
       " \"'\",\n",
       " 's',\n",
       " 'NLP',\n",
       " 'Tutorial',\n",
       " '.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'stick',\n",
       " 'with',\n",
       " 'this',\n",
       " 'notebook',\n",
       " '.',\n",
       " 'I',\n",
       " 'have',\n",
       " 'made',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'effort',\n",
       " 'to',\n",
       " 'create',\n",
       " 'it']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c8b556-4f92-4ac8-9c84-e9631ab902e3",
   "metadata": {},
   "source": [
    "#### Here we can see in word_tokenize and wordpunct_tokenize , is in wordpunct_tokenize is that ' is also considered as seperate word in wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25d668b0-a839-4a0a-9d03-7bd18e59e648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'Subrat',\n",
       " 'Bahuguna',\n",
       " \"'s\",\n",
       " 'NLP',\n",
       " 'Tutorial.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'stick',\n",
       " 'with',\n",
       " 'this',\n",
       " 'notebook.',\n",
       " 'I',\n",
       " 'have',\n",
       " 'made',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'effort',\n",
       " 'to',\n",
       " 'create',\n",
       " 'it']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tb_token = TreebankWordTokenizer()\n",
    "tb_token.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ab88d5-9c22-476d-b1ff-126481300b8e",
   "metadata": {},
   "source": [
    "### Here we can see in Treebankword . is not considered as a token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef41793-f4f4-43d0-8de8-ac39854feb52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
